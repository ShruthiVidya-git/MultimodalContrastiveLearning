{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sample.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPZ391u5fzC6lwWtp/l/nP+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShruthiVidya-git/MultimodalContrastiveLearning/blob/main/Sample/Sample.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ">Executable file for sample data\n",
        "\n",
        "This code is adopted from Gloria Repository"
      ],
      "metadata": {
        "id": "nqLACl2084cK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUW5j1ox83-3",
        "outputId": "be06e67f-2528-49c9-830b-d7da4cac1b99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.21.2-py3-none-any.whl (4.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.7 MB 4.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 41.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.9.0-py3-none-any.whl (120 kB)\n",
            "\u001b[K     |████████████████████████████████| 120 kB 58.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.9.0 tokenizers-0.12.1 transformers-4.21.2\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys, torch.nn as nn, torch, torchvision, pickle, os, pathlib, re, numpy as np, pandas as pd, glob, gc,numpy as np, pandas as pd, random, os, warnings, cv2\n",
        "from torchvision import models as models_2d\n",
        "from sklearn import metrics\n",
        "from tqdm import tqdm\n",
        "from torch.autograd import Variable\n",
        "from transformers import AutoTokenizer, BertModel, AutoModel\n",
        "\n",
        "random.seed(500)"
      ],
      "metadata": {
        "id": "rjnLgcH6FJYH"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VisionTransformer( nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VisionTransformer,self).__init__()\n",
        "\n",
        "        # specificatoins for vit\n",
        "         \n",
        "        self.patch_size = 32\n",
        "        self.num_channels = 3\n",
        "        self.num_heads = 8\n",
        "        self.embed_dim = 768\n",
        "        self.hidden_dim = 512\n",
        "        self.num_patches = (256 // self.patch_size) ** 2\n",
        "        self.dropout= 0.1\n",
        "        self.num_layers = 6\n",
        "        \n",
        "        # Layers/Networks\n",
        "        self.input_layer = nn.Linear(self.num_channels*(self.patch_size**2), self.embed_dim)\n",
        "\n",
        "        self.layer_norm_1 = nn.LayerNorm(self.embed_dim)\n",
        "        self.attn = nn.MultiheadAttention(self.embed_dim, self.num_heads,\n",
        "                                          dropout=self.dropout)\n",
        "        self.layer_norm_2 = nn.LayerNorm(self.embed_dim)\n",
        "        self.linear = nn.Sequential(\n",
        "            nn.Linear(self.embed_dim, self.hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(self.dropout),\n",
        "            nn.Linear(self.hidden_dim, self.embed_dim),\n",
        "            nn.Dropout(self.dropout))\n",
        "       \n",
        "        self.mlp_head = nn.Sequential(\n",
        "            nn.LayerNorm(self.embed_dim)\n",
        "        )\n",
        "        self.dropout = nn.Dropout(self.dropout)\n",
        "\n",
        "        # Parameters/Embeddings\n",
        "        self.cls_token = nn.Parameter(torch.randn(1,1,self.embed_dim))\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1,1+self.num_patches,self.embed_dim))\n",
        "\n",
        " \n",
        "    #image to patch\n",
        "    def img_to_patch(self, x, patch_size = 32, flatten_channels=True):\n",
        "        B, C, H, W = x.shape\n",
        "        x = x.reshape(B, C, H//patch_size, patch_size, W//patch_size, patch_size)\n",
        "        x = x.permute(0, 2, 4, 1, 3, 5) # [B, H', W', C, p_H, p_W]\n",
        "        x = x.flatten(1,2)              # [B, H'*W', C, p_H, p_W]\n",
        "        if flatten_channels:\n",
        "                x = x.flatten(2,4)          \n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "    def encode(self, x):\n",
        "        # Preprocess input\n",
        "        x = self.img_to_patch(x, self.patch_size)\n",
        "        B, T, _ = x.shape\n",
        "        x = self.input_layer(x)\n",
        "\n",
        "        # Add CLS token and positional encoding\n",
        "        cls_token = self.cls_token.repeat(B, 1, 1)\n",
        "        x = torch.cat([cls_token, x], dim=1)\n",
        "        x = x + self.pos_embedding[:,:T+1]\n",
        "        x1 = x\n",
        "        # Apply Transforrmer\n",
        "        x = self.dropout(x)\n",
        "        x = x.transpose(0, 1)\n",
        "        inp_x = self.layer_norm_1(x)\n",
        "        x = x + self.attn(inp_x, inp_x, inp_x)[0]\n",
        "        x = x + self.linear(self.layer_norm_2(x)) \n",
        "        inp_x = self.layer_norm_1(x)\n",
        "        x = x + self.attn(inp_x, inp_x, inp_x)[0]\n",
        "        x = x + self.linear(self.layer_norm_2(x))\n",
        "        inp_x = self.layer_norm_1(x)\n",
        "        x = x + self.attn(inp_x, inp_x, inp_x)[0]\n",
        "        x = x + self.linear(self.layer_norm_2(x))\n",
        "        inp_x = self.layer_norm_1(x)\n",
        "        x = x + self.attn(inp_x, inp_x, inp_x)[0]\n",
        "        x = x + self.linear(self.layer_norm_2(x))\n",
        "        inp_x = self.layer_norm_1(x)\n",
        "        x = x + self.attn(inp_x, inp_x, inp_x)[0]\n",
        "        x = x + self.linear(self.layer_norm_2(x))\n",
        "        inp_x = self.layer_norm_1(x)\n",
        "        x = x + self.attn(inp_x, inp_x, inp_x)[0]\n",
        "        x = x + self.linear(self.layer_norm_2(x))\n",
        "        cls = x[0]\n",
        "        return cls\n",
        "        \n",
        "    \n",
        "    #similarity global function \n",
        "    def get_global_similarities( self, img_emb_g, text_emb_g):\n",
        "        img_emb_g = img_emb_g.detach().cpu().numpy()\n",
        "        text_emb_g = text_emb_g.detach().cpu().numpy()\n",
        "        global_similarities = metrics.pairwise.cosine_similarity(img_emb_g, text_emb_g)\n",
        "        global_similarities = torch.Tensor(global_similarities*10)\n",
        "        return global_similarities"
      ],
      "metadata": {
        "id": "6kqPOiC-H06M"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__==\"__main__\":   \n",
        "    checkpoints_path = '/content/checkpoint_state_dict.pt'\n",
        "    img_path = '/content/Sample Image.jpg'\n",
        "    prompts_path = '/content/class_prompts_embeddings.pickle'\n",
        "\n",
        "    # import and load checkpoints\n",
        "    vit = VisionTransformer()\n",
        "    checkpoint = torch.load(checkpoints_path)\n",
        "    vit.load_state_dict(checkpoint)\n",
        "\n",
        "    # read, resize & normalize image\n",
        "    sample = cv2.imread(img_path)\n",
        "    sample_img = cv2.resize(sample, (256,256),interpolation = cv2.INTER_CUBIC)\n",
        "    sample_img = (sample_img - np.min(sample_img)) / (np.max(sample_img) - np.min(sample_img)) \n",
        "    sample_img = torch.reshape(torch.tensor(sample_img), (3,256,256)).unsqueeze(0).type(torch.FloatTensor)\n",
        "\n",
        "    # get encoded image \n",
        "    img_g = vit.encode(sample_img)\n",
        "\n",
        "    # read class prompt embeddings from pickle file\n",
        "    with open(prompts_path, \"rb\") as f:\n",
        "        promp_embeddings = pickle.load(f)\n",
        "\n",
        "    text_g = promp_embeddings['global_embed']['Cardiomegaly'].unsqueeze(0)\n",
        "    similarity = vit.get_global_similarities(img_g, text_g)\n",
        "    threshold = 0\n",
        "    if similarity > threshold:\n",
        "        print('Predicted class is Cardiomegaly ! ')\n",
        "    else:\n",
        "        print('No Findings')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-YccBpVKdrM",
        "outputId": "294fb0fb-7356-41a8-cf8c-432cdaa0f8ae"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted class is Cardiomegaly ! \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dQtjpZk5VR1m"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}